---
title: "Análisis estadístico de base de datos broadway-shows"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caTools)
```

César Alfonso Rosario Escobar 15-11295

Giacomo Leonardo Tosone Ohep 14-11085

Joaquin Alejandro Yañez Moreno 17-10678

Roimar Alejandro Flores López 17-10196


### Planteamiento del Problema

Los espectáculos de Broadway han sido un medio de entrenamiento artístico que ha estado presente a lo largo de muchos años, con recaudaciones millonarias a lo largo de los años. Es claro el auge y desplazamiento que han generado los nuevos medios de entretenimiento, los cuales han tenido un crecimiento exponencial, atrayendo cada vez más y más espectadores. 
	
En base a esto, este proyecto tiene como objetivo utilizar datos sobre los espectáculos de Broadway para analizar el comportamiento que ha tenido este medio entre 1984 y 2016.
	
### Objetivo General

Aplicar herramientas estadísticas para medir la transformación que han tenido los espectáculos de Broadway en el período 1984-2016.

### Objetivos Específicos

- Determinar datos sobre la población a estudiar por medio de análisis estadístico.

- Examinar los resultados obtenidos a través de observaciones realizadas a los datos.

- Tabular y graficar cuanto sea necesario para justificar e ilustrar observaciones e inferencias sobre los datos presentados.

### Descripción de la Base de Datos

Los datos proporcionados consisten de información cualitativa referente a elementos comunes del espectáculo como: el dinero recaudado, la concurrencia, las semanas de actividad, el año, las producciones nuevas así como los tickets que no se han vendido y el precio de cada uno.

### Metodología

Para alcanzar lo planteado se emplearán las siguientes técnicas de estadística inferencial y descriptiva: Intervalos de confianza, pruebas de hipótesis, pruebas de bondad de ajuste **inserte más técnicas concretas a usar en los distintos ejercicios**



# Análisis descriptivo

## Datos en cuestión

```{r}
project_data <- read.delim("broadway-shows.txt")
str(project_data)
```


## Información de recaudo (Millones de dólares)

### Medidas de tendencia central

```{r}
var(project_data$Gross..M.)
sd(project_data$Gross..M.)
```

### Medidas de dispersion

```{r}
summary(project_data$Gross..M.)
```

De aquí resulta inmediato que 
* Los datos están un poco más dispersos entre los dos primeros cuartiles, así que hay una leve asimetría.
* La mediana se aleja considerablemente de la media (48.7 millones), pero se sabe que esto se ve afectado por la
  sensibilidad de la mediana a valores altos.
* Si bien no hay valores atipicos el bigote superior está bastante alto.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$Gross..M.,main="Histograma de recaudo",xlab="Recaudo (millones de $)",ylab="frecuencia de recaudo",ylim = c(0,10),breaks=6)
recaudo_bp <- boxplot(project_data$Gross..M.,main="Diagrama de caja de recaudo",ylab="recaudo (millones de $)")
```

Como se observo del resumen anterior, la dispersión en los datos entre los cuartiles extremos es casi la misma. 


## Información de concurrencia (miles de personas)

### Medidas de tendencia central

```{r}
 mean(project_data$Attendance)
 median(project_data$Attendance)
 var(project_data$Attendance)
 sd(project_data$Attendance)
```

Como se puede observar los shows tenían una asistencia media de 10473, valor que oscilaba principalmente entre 8326 y 12621 personas. Además, no hay presencia de datos extremas debido a que la diferencia entre la media y la mediana es pequeña.

### Medidas de dispersion

```{r}
summary(project_data$Attendance)
IQR(project_data$Attendance)
```

En este caso hay un rango intercuartil de 4.04 lo que lleva a concluir que no hay una gran dispersión en el conjunto de datos.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$Attendance,main="Histograma de concurrencia",xlab="miles de personas",ylab="frecuencia de la concurrencia")
recaudo_bp <- boxplot(project_data$Attendance,main="Diagrama de caja de concurrencia",ylab="miles de personas")
```

De acuerdo a las gráficas se observa que la frecuencia mas alta se encuentra en el rango de las 11000 personas y la menor en el rango de las 6000.

## Semanas activas para las obras

### Medidas de tendencia central

```{r}
mean(project_data$Playing.weeks)
median(project_data$Playing.weeks)
var(project_data$Playing.weeks)
sd(project_data$Playing.weeks)
```

Con respecto a la cantidad de semanas activas se detalla una media de 1340 y una mediana de 1442, por lo que a primera vista no existe una presencia de valores atípicos.

### Medidas de dispersion

```{r} 
summary(project_data$Playing.weeks) 
IQR(project_data$Playing.weeks) 
``` 

Para las semanas de actividad se obtuvo un rango intercuartil de 400, lo que señala que la 50% intermedio esta comprendido entre 1100 y 1500 lo que indica cierto nivel de dispersión.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$Playing.weeks,main="Histograma de Semanas activas",xlab="Semanas activas")
recaudo_bp <- boxplot(project_data$Playing.weeks,main="Diagrama de caja de ",ylab="Semanasa activas")
```

De acuerdo a las gráficas se observa la poca frecuencia de elementos alrededor de la mediana, y que la mayoría de los datos se concentran a partir del tercer cuartil.

## Número de obras nuevas 

### Medidas de tendencia central

```{r}
mean(project_data$New.Productions)
median(project_data$New.Productions)
var(project_data$New.Productions)
sd(project_data$New.Productions)
```

El número de obras nuevas oscila entre 33 y 41 con una media de 37.36, cabe destacar que la media es prácticamente igual que la mediana, por lo que en este caso no hay valores extremos.

### Medidas de dispersion

```{r}
summary(project_data$New.Productions)
IQR(project_data$New.Productions)
```

En cuanto a la dispersion es claro que los cuantiles poseen distancias parecidas con un rango intercuartil de 5, lo que indica que los datos están concentrados en la zona central.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$New.Productions,main="Histograma de # de obras nuevas",xlab="# de obras nuevas")
recaudo_bp <- boxplot(project_data$New.Productions,main="Diagrama de caja de obras nuevas",ylab="# de obras nuevas")
```

Gracias a la gráficas se confirma la afirmación anterior dado que claramente los datos se encuentran principalmente en el centro.

## Precio promedio de los tickets (millones de dólares)

### Medidas de tendencia central

```{r}
mean(project_data$Mean.ticket)
median(project_data$Mean.ticket)
var(project_data$Mean.ticket)
sd(project_data$Mean.ticket)
```

En este caso se tiene una media de 61.65 con un rango de valores mucho mayor que va desde 36 hasta 87.

### Medidas de dispersion

```{r}
summary(project_data$Mean.ticket)
IQR(project_data$Mean.ticket)
```

Para el precio del ticket el rango intercuartil es bastante alto con 33.88 lo que indica una alta dispersión de los datos.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$Mean.ticket,main="precio promedio de tickets",xlab="precio promedio de tickets")
recaudo_bp <- boxplot(project_data$Mean.ticket,main="precio promedio de tickets",ylab="precio promedio de tickets")
```

En las gráfica se observa cierta uniformidad excluyendo el rango de los 40.

## Porcentaje de tickets que no se venden

### Medidas de tendencia central

```{r}
mean(project_data$Pct.sold)
median(project_data$Pct.sold)
var(project_data$Pct.sold)
sd(project_data$Pct.sold)
```

### Medidas de dispersion

```{r}
summary(project_data$Pct.sold)
IQR(project_data$Pct.sold)
```

Hay una gran diferencia entre el mínimo y el primer cuartil con un rango intercuartil de 0.0033. Indica una concentración de los datos entre el tercer cuartil y el máximo.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$Pct.sold,main="% de tickets no vendidos",xlab="% no vendido")
recaudo_bp <- boxplot(project_data$Pct.sold,main="% de tickets no vendidos")
```

Se observa claramente la baja frecuencia de valores en la primera mitad de la gráfica y la alta concentración en la segunda mitad.

## Logaritmo de lo recaudado.

### Medidas de tendencia central

```{r}
mean(project_data$LogGross)
median(project_data$LogGross)
var(project_data$LogGross)
sd(project_data$LogGross)
```

Para el logaritmo de las ganancias se detalla poca diferencia entre la media y la mediana, por lo que no se indica presencia de valores extremos.

### Medidas de dispersion

```{r}
summary(project_data$LogGross)
IQR(project_data$LogGross)
```

La distancia entre los cuartiles de la primera mitad es mayor que los de la segunda mitad.

```{r}
par(mfrow= c(1,2))
recaudo_hist <- hist(project_data$LogGross,main="Logaritmo de recaudo",xlab="Logaritmo")
recaudo_bp <- boxplot(project_data$LogGross,main="Logaritmo")
```

Respaldando la observación anterior se observa la concentración uniforme de los elementos a partir de la mitad de la gráfica.


# Intervalos de confianza

El tamaño de los datos es 33.

## Información de recaudo (Millones de dólares)

```{r}
t.test(project_data$Gross..M., conf.level = 0.97)$conf.int
```

-- Análisis

## Información de concurrencia (miles de personas)

```{r}
t.test(project_data$Attendance, conf.level = 0.97)$conf.int
```

-- Análisis

## Semanas activas para las obras

```{r}
t.test(project_data$Playing.weeks, conf.level = 0.97)$conf.int
```

-- Análisis

## Número de obras nuevas

```{r}
t.test(project_data$New.Productions, conf.level = 0.97)$conf.int
```

-- Análisis

## Precio promedio de los tickets

```{r}
t.test(project_data$Mean.ticket, conf.level = 0.97)$conf.int
```

-- Análisis

## Porcentaje de tickets que no se venden

```{r}
t.test(project_data$Pct.sold, conf.level = 0.97)$conf.int
```

-- Análisis

## Logaritmo de lo recaudado.

```{r}
t.test(project_data$LogGross, conf.level = 0.97)$conf.int
```

-- Analisis


# Análisis sobre el promedio de obras nuevas

Se realizan los calculos necesarios para obtener el estadistico de prueba y analizarlo en relación a la región de rechazo.

Media y varianzas muestrales:
```{r}
act_mean <- mean(project_data$New.Productions)
act_sd   <- sd(project_data$New.Productions)

act_mean
act_sd
```

Estimador de prueba:
```{r}
test_estimator <- (act_mean - 40)/act_sd
test_estimator
```

Como se pide ver que la media sea menor a 40, se asume que es igual y la region de rechazo será de cola inferior (Z < z_alpha). Luego, se busca  el cuantil asociado a la probabilidad de 0.05 para una distribucion normal, y se comparan los resultados.
```{r}
alpha <- qnorm(0.05)

alpha
```

Se observa  que el estadistico de prueba cae dentro de la región de rechazo. Por lo tanto hay suficiente evidencia para asumir que de hecho el promedio de obras nuevas se encuentra por debajo de 40.

Ahora, se observa el p-valor asociado a nuestra prueba para profundizar nuestro análisis.
```{r}
p_val <- t.test(project_data$New.Productions,mu=40,alternative="less")
p_val
```

De estos resultados se concluye que la prueba deberia tener una significancia menor al 0.06% para ser aceptada, lo quiere decir que en la práctica, siempre se rechazará la hipotesis nula.


## Prueba chi-cuadrado para determinar si el recaudo se distribuye de forma normal

Se parte de la hipotésis de que el recaudo en millones de dólares entre 1984 y 2016 tiene distribución normal. Además, el ajuste se basará en un nivel de 5%.

Primero, se agrupan los datos en clases con tamaños convenientes:

```{r}
gross_hist <- hist(project_data$Gross..M.,plot=F,breaks=seq(0,1500,300))
gross_hist
```

Se unen las dos últimas clases para cumplir la precondición de # de frecuencia para bondad de ajuste.



```{r}
gross_freqs <- c(8,7,7,11)
gross_mids  <- c(150,450,750,1200)

n <- sum( gross_freqs )
```

Ahora que se cuenta con los datos agrupados, se observa si se asimilan a una distribución normal.

```{r}
qqnorm(gross_freqs)
qqline(gross_freqs)
```

El diagrama cuantil-cuantil indica que de hecho los datos no se asimilan a una distribución normal. Aún así se procede ahora a obtener valores para la media y varianza muestrales (cuyo calculo hace perder 2 grados de libertad).

```{r}
gross_grouped_mean <- sum(gross_freqs * gross_mids)/n

rep_mean           <- rep(gross_grouped_mean,4)
gross_grouped_var  <- (sum(gross_freqs - rep_mean)^2)/(n-1)

gross_grouped_sd   <- sqrt(gross_grouped_var)

gross_grouped_mean
gross_grouped_sd
```


Se calculan las probabilidades asociadas a cada clase:

```{r}
lower_bound <- c(0,300,600,900)
upper_bound <- c(300,600,900,1500)

p1 <- pnorm(lower_bound, gross_grouped_mean, gross_grouped_sd)
p2 <- pnorm(upper_bound, gross_grouped_mean, gross_grouped_sd)

class_probs <- p2-p1
class_probs
```

Y por último se obtiene el estimador chi cuadrado necesario para analizar la región de rechazo.

```{r}
est_chisq <- sum( (gross_freqs - n*class_probs)^2/(n*class_probs) )
lim_rr    <- qchisq(0.95,2)

est_chisq
lim_rr
```

El estadístico hallado no está en la región de rechazo, por lo que la evidencia sugiere que el recaudo en millones de dólares se distribuye normalmente. Esto concuerda con la simetría descrita en el boxplot, en la sección 1 sobre la dispersión de los datos del recaudo.

Si se observa ahora el pvalor

```{r}
pchisq(est_chisq,2,lower.tail=F)
```

Es claro que es bastante alto, lo suficiente para aceptar la hipotesis de normalidad a cualquier nivel.


# Gráficos de dispersión 

```{r}
pairs(project_data)
```
En el grafico de dispersión podemos observar que existe relación entre las variables, a excepción de porcentaje de tickets que no se venden, y especialmente en el número de obras nuevas, donde no se ve ningún tipo de patrón.

# Matriz de correlación

```{r}
cor(project_data)
```
De nuevo en la matriz de correlación podemos observar como se presentan valores altos en casi todos los campos, excepto en pct.sold y new.productions con valores por debajo de 0.65 con niveles de hasta aproximadamente 0.35.

## Muestreo de datos

Se generaran dos nuevos conjuntos de datos, uno que contiene el 80% de los datos originales y otro el 20%.

```{r}

set.seed(101) 

data_size <- length( project_data$Season )
n_vars    <- length(project_data)
var_names <- colnames(project_data)


sample_result <- sample.split(project_data$Gross..M., SplitRatio = 4/5)

train_size    <- round( data_size * 0.8 )
validate_size <- round( data_size * 0.2 )

train_df    <- data.frame( matrix( rep(0,train_size * n_vars), nrow = train_size) )
validate_df <- data.frame( matrix( rep(0,validate_size * n_vars), nrow = validate_size))

for (i in 1:n_vars) { 
  train_df[i]    <- subset(project_data[i], sample_result == T) 
  validate_df[i] <- subset(project_data[i], sample_result == F)
}

colnames(train_df)    <- var_names
colnames(validate_df) <- var_names
```


# Modelo para el recaudo en millones de dólares

Se buscará un buen modelo para el recaudo. Para esto se realizará una regresión paso a paso.

El modelo inicial es:

```{r}
mod_gross_1 <- lm(train_df$Gross..M. ~ train_df$Attendance + train_df$Playing.weeks
                  + train_df$New.Productions + train_df$Mean.ticket
                  + train_df$Pct.sold)

summary(mod_gross_1)
```

Se observa que la mayoría de las variables son significativas a casi cualquier nivel, a excepción de las nuevas producciones, que aun es significativa a un nivel aceptable. Se decide removerla para mejorar el modelo.

```{r}
mod_gross_2 <- lm(train_df$Gross..M. ~ train_df$Attendance + train_df$Playing.weeks
                  + train_df$Mean.ticket + train_df$Pct.sold)

summary(mod_gross_2)
```
Ahora todas las variables son significativas a casi cualquier nivel y la disminución en el R-cuadrado ajustado es despreciable.

Se estudian ahora los residuos:
```{r}
par(mfrow=c(2,2))
plot(mod_gross_2)
```

+ Residuos vs Ajustados: Los datos no presentan tendencia alguna por lo que se garantiza la independencia.
+ QQplot: La mayoria de los datos se ajustan a la recta salvo 3 excepciones. Esto concuerda con el resultado obtenido en la prueba de bondad de ajuste.
+ Escala-localización: Los datos se presentan como una nube asi que hay garantía de homocedasticidad.
+ Residuos vs Apalancamiento: El octavo dato parece estar ejerciendo una leve influencia sobre el resto.

# Modelo para el logaritmo del recaudo en millones de dólares

Se realizará ahora un modelo para el logaritmo del recaudo, que se comparará con el anterior. 

De nuevo, se realiza una regresión paso a paso.

```{r}
mod_lgross_1 <- lm(train_df$LogGross ~ train_df$Attendance + train_df$Playing.weeks 
            + train_df$New.Productions + train_df$Mean.ticket + train_df$Pct.sold)
summary(mod_lgross_1)
```

Se elimina, de nuevo, la variable correspondiente a las nuevas producciones.

```{r}
mod_lgross_2 <- lm(train_df$LogGross ~ train_df$Attendance + train_df$Playing.weeks 
             + train_df$Mean.ticket + train_df$Pct.sold)
summary(mod_lgross_2)
```

Esta vez se elimina también el intercepto. 

```{r}
mod_lgross_3 <- lm(LogGross ~ Attendance + Playing.weeks 
             + Mean.ticket + Pct.sold - 1, data=train_df)
summary(mod_lgross_3)
```

En este caso se observa que todas las variables del modelo son significativas a cualquier nivel y que el R-cuadrado ajustado es de 1 , un valor bastante deseable.

Se estudian ahora los residuos de este modelo.

```{r}
par(mfrow=c(2,2))
plot(mod_lgross_3)
```

+ Residuos vs Ajustados: No hay tendencia marcada, se garantiza independencia.
+ QQplot: Los residuos presentan normalidad, salvo 3 exepciones no demasiado alejadas.
+ Escala-Localización: Los datos observados no muestran tendencia, se garantiza homocedasticidad.
+ Residuos vs Apalancamiento: Ningún dato influencia la tendencia del resto en este modelo.

Dados los niveles de significancia obtenidos para cada variable  y los valores asociados a los residuos en ambos modelos, se escoge estudiar el logaritmo del recaudo.

# Prediccion del logaritmo del recaudo

Se procede a predecir valores para el logaritmo del recaudo para el grupo de datos escogido para tal fin.

```{r}
predicction <- predict(mod_lgross_3, validate_df, interval="prediction")
predicction
```

Se comparan entonces los valores ajustados por el modelo con los valores reales del 20% de los datos

```{r}
summary(validate_df$LogGross - predicction[,1])
boxplot(validate_df$LogGross - predicction[,1])
```
